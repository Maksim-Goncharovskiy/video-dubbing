{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44328e26",
   "metadata": {},
   "source": [
    "# <center>Video Dubbing Full Pipeline</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c90c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import torch \n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from moviepy import VideoFileClip, AudioFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653ebb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_from_mp4(video_path: str, target_sr: int = 16000, temp_dir='./temp-audios', delete_file=True) -> tuple[np.ndarray, int]:\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio: AudioFileClip = video.audio\n",
    "    \n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "\n",
    "    temp_audio_path = temp_dir + \"/temp_audio.wav\"\n",
    "    audio.write_audiofile(temp_audio_path, codec='pcm_s16le', fps=target_sr)\n",
    "    \n",
    "    audio_data, sr = torchaudio.load(temp_audio_path)\n",
    "    \n",
    "    if sr != target_sr:\n",
    "        audio_data = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(audio_data)\n",
    "    \n",
    "    if audio_data.shape[0] > 1:\n",
    "        audio_data = audio_data.mean(dim=0)\n",
    "\n",
    "    if delete_file:\n",
    "        os.remove(temp_audio_path)\n",
    "    \n",
    "    return audio_data.numpy(), sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d46adf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./temp-audios/temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "video_path = \"/home/maksim/Repos/video-dubbing/test-videos/videoplayback.mp4\"\n",
    "\n",
    "audio, sr = extract_audio_from_mp4(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91802f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DubbingSegment:\n",
    "    start: int = field(repr=True) # Начало сегмента (индекс в исходном аудио)\n",
    "    end: int = field(repr=True) # Конец сегмента\n",
    "    audio: np.ndarray = field(repr=False) # Аудио сегмент\n",
    "\n",
    "    transcription: str = field(repr=True, default=None)\n",
    "    translation: str = field(repr=True, default=None)\n",
    "\n",
    "    tts_wav: np.ndarray = field(repr=False, default=None)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingContext:\n",
    "    original_audio: np.ndarray = field(repr=False)\n",
    "    sample_rate: int = field(repr=True)\n",
    "    temp_dir: str = field(repr=True)\n",
    "\n",
    "    segments: list[DubbingSegment] = field(default_factory=list, repr=False)\n",
    "\n",
    "    speech_audio: np.ndarray = field(repr=False, default=None)\n",
    "\n",
    "    timestamps_mapping: dict[tuple[int, int], DubbingSegment] = field(repr=False, default=None) # соответствие временных отрезков между original_audio и speech_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b18ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class BaseProcessor(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, context: ProcessingContext) -> ProcessingContext:\n",
    "        pass\n",
    "\n",
    "\n",
    "class VADProcessor(BaseProcessor):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ASRProcessor(BaseProcessor):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MTProcessor(BaseProcessor):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TTSProcessor(BaseProcessor):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c58ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class SileroVADProcessor(VADProcessor):\n",
    "    def __init__(self, model_path: str = \"\", threshold: float = 0.5,  \n",
    "                  min_silence_duration_ms=1000, \n",
    "                  min_speech_duration_ms=1000):\n",
    "        \"\"\"\n",
    "        Для локальной загрузки модели, нужно сначала её скачать: git clone <silerovad repo>\n",
    "        А затем передать в качестве параметра model_path путь до корня склонированного репозитория.\n",
    "        \"\"\"\n",
    "        if model_path:\n",
    "            self.silerovad, utils = torch.hub.load(repo_or_dir=model_path,\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              source='local')\n",
    "        else:\n",
    "            self.silerovad, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                                model='silero_vad',\n",
    "                                force_reload=True, \n",
    "                                source='github') \n",
    "\n",
    "        (self.get_speech_timestamps, _, _, _, _) = utils\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.min_silence_duration_ms = min_silence_duration_ms\n",
    "        self.min_speech_duration_ms = min_speech_duration_ms\n",
    "\n",
    "\n",
    "    def __call__(self, context: ProcessingContext) -> ProcessingContext:\n",
    "        speech_audio = []\n",
    "        new2old = {}\n",
    "    \n",
    "        speech_timestamps = self.get_speech_timestamps(context.original_audio, \n",
    "                                                  self.silerovad, \n",
    "                                                  threshold=self.threshold, \n",
    "                                                  sampling_rate=context.sample_rate,\n",
    "                                                  min_silence_duration_ms=self.min_silence_duration_ms, \n",
    "                                                  min_speech_duration_ms=self.min_speech_duration_ms)\n",
    "    \n",
    "        for ts in speech_timestamps:\n",
    "            start = ts['start']\n",
    "            end = ts['end']\n",
    "            context.segments.append(DubbingSegment(start=start, \n",
    "                                                 end=end, \n",
    "                                                 audio=context.original_audio[start:end]))\n",
    "\n",
    "\n",
    "        start_idx = 0\n",
    "\n",
    "        for segment in context.segments:\n",
    "            new2old[(start_idx, segment.end - segment.start + start_idx)] = segment\n",
    "\n",
    "            start_idx = segment.end - segment.start + start_idx + 1\n",
    "\n",
    "            speech_audio.extend(segment.audio.tolist())\n",
    "\n",
    "        context.speech_audio = np.array(speech_audio)\n",
    "        context.timestamps_mapping = new2old\n",
    "        \n",
    "        return context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf52848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ProcessingContext(original_audio=audio, sample_rate=sr, temp_dir=\"./temp-dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d92156",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_path = \"/home/maksim/Models/SileroVAD/snakers4-silero-vad\"\n",
    "\n",
    "vad_pipe = SileroVADProcessor(vad_path)\n",
    "\n",
    "context = vad_pipe(context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3222cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "\n",
    "class FasterWhisperProcessor(ASRProcessor):\n",
    "    sr = 16_000\n",
    "\n",
    "    def __init__(self, model_size_or_path: str = \"tiny.en\", device: str = \"cpu\", compute_type: str = \"int8\", batch_size=8):\n",
    "        self.model = model=WhisperModel(model_size_or_path=model_size_or_path,\n",
    "                                                                  device=device, \n",
    "                                                                  compute_type=compute_type)\n",
    "        self.batch_size = batch_size\n",
    "        self.last_segment: DubbingSegment | None = None\n",
    "\n",
    "\n",
    "    def _put_word_in_segment(self, word: str, start: int, end: int, context: ProcessingContext):\n",
    "        word_processed: bool = False\n",
    "\n",
    "        for ts_interval in context.timestamps_mapping.keys():\n",
    "            if start >= ts_interval[0] and end <= ts_interval[1]:\n",
    "                segment = context.timestamps_mapping[ts_interval]\n",
    "                if segment.transcription:\n",
    "                    segment.transcription += word\n",
    "                else:\n",
    "                    segment.transcription = word\n",
    "\n",
    "                word_processed = True\n",
    "                self.last_segment = segment\n",
    "\n",
    "                break \n",
    "\n",
    "        if not(word_processed):\n",
    "            self.last_segment.transcription += word\n",
    "        \n",
    "        return context\n",
    "\n",
    "    \n",
    "    def __call__(self, context: ProcessingContext) -> ProcessingContext:\n",
    "        speech_audio = context.speech_audio\n",
    "\n",
    "        if context.sample_rate != self.sr:\n",
    "            speech_audio = torchaudio.transforms.Resample(orig_freq=context.sample_rate,\n",
    "                                                           new_freq=self.sr)(torch.tensor(speech_audio)).numpy()\n",
    "\n",
    "        whisper_segments, _ = self.model.transcribe(context.speech_audio, word_timestamps=True)\n",
    "        \n",
    "        for segment in whisper_segments:\n",
    "            for word in segment.words:\n",
    "                context = self._put_word_in_segment(word=word.word,\n",
    "                                           start=int(word.start * self.sr), \n",
    "                                           end=int(word.end * self.sr), \n",
    "                                           context=context)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eac322be",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_path = \"/home/maksim/Models/FasterWhisper/tiny-en\"\n",
    "\n",
    "asr_pipe = FasterWhisperProcessor(whisper_path)\n",
    "\n",
    "context = asr_pipe(context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861c8afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" What you're doing right now at this very moment is killing you.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.segments[0].transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d410f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "class HelsinkiEnRuProcessor(MTProcessor):\n",
    "    def __init__(self, model_path: str | None = None, device: str = 'cpu'):\n",
    "        model = None\n",
    "        tokenizer = None \n",
    "        if model_path:\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        else:\n",
    "            model_hf_name = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_hf_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_hf_name)\n",
    "        \n",
    "        self.pipe = pipeline(\n",
    "            task=\"translation\", \n",
    "            model=model, \n",
    "            tokenizer=tokenizer,\n",
    "            device=device)\n",
    "\n",
    "\n",
    "    def _process_sample(self, text_en: str) -> str:\n",
    "        return self.pipe(text_en)[0]['translation_text']\n",
    "\n",
    "\n",
    "    def __call__(self, context: ProcessingContext) -> ProcessingContext:\n",
    "        for segment in context.segments:\n",
    "            segment.translation = self._process_sample(segment.transcription)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "409fe934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksim/miniconda3/envs/video_dubbing/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "mt_path = \"/home/maksim/Models/OpusEnRu\"\n",
    "\n",
    "mt_pipe = HelsinkiEnRuProcessor(model_path=mt_path)\n",
    "\n",
    "context = mt_pipe(context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b16d4e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "То, что ты сейчас делаешь, убивает тебя.\n"
     ]
    }
   ],
   "source": [
    "print(context.segments[0].translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348dbe21",
   "metadata": {},
   "source": [
    "# TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86f14f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0fb586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "class XTTSProcessor(TTSProcessor):\n",
    "    output_sample_rate = 24_000\n",
    "\n",
    "    def __init__(self, target_spk: str | None = None, model_path: str | None = None, device: str = 'cpu', temp_dir=\"./temp-dir/tts/\"):\n",
    "        self.target_spk = target_spk\n",
    "\n",
    "        if model_path:\n",
    "            self.model = TTS(model_path=model_path, config_path=f'{model_path}/config.json').to(device)\n",
    "        else:\n",
    "            self.model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "        self.temp_dir = temp_dir\n",
    "\n",
    "\n",
    "    def _process_sample(self, text_ru: str, speaker_wav: str) -> np.ndarray:\n",
    "        return self.model.tts(text=text_ru, speaker_wav=speaker_wav, language='ru')\n",
    "\n",
    "\n",
    "    def __call__(self, context: ProcessingContext) -> ProcessingContext:\n",
    "        os.makedirs(self.temp_dir, exist_ok=True)\n",
    "\n",
    "        for i, segment in enumerate(context.segments):\n",
    "            if self.target_spk is None:\n",
    "                audio_path = self.temp_dir + f\"{i}.wav\"\n",
    "\n",
    "                sf.write(audio_path, segment.audio, 16_000)\n",
    "\n",
    "                segment.tts_wav = self._process_sample(segment.translation, audio_path)\n",
    "\n",
    "                os.remove(audio_path)\n",
    "            else:\n",
    "                segment.tts_wav = self._process_sample(segment.translation, self.target_spk)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbae7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SileroTTSProcessor(TTSProcessor):\n",
    "    output_sample_rate = 48_000\n",
    "\n",
    "    def __init__(self, model_path: str | None = None, device: str = 'cpu', speaker: str = \"xenia\"):\n",
    "        if model_path:\n",
    "            self.silero_tts, _ = torch.hub.load(repo_or_dir=model_path,\n",
    "                                     model='silero_tts',\n",
    "                                     language='ru',\n",
    "                                     speaker='v4_ru',\n",
    "                                     source='local')\n",
    "\n",
    "        else:\n",
    "            self.silero_tts, _ = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                     model='silero_tts',\n",
    "                                     language='ru',\n",
    "                                     speaker='v4_ru',\n",
    "                                     source='github')\n",
    "        \n",
    "        self.silero_tts.to(device)\n",
    "\n",
    "        self.speaker = speaker\n",
    "\n",
    "\n",
    "    def _process_sample(self, text_ru: str, speaker: str) -> np.ndarray:\n",
    "        return self.silero_tts.apply_tts(text=text_ru,\n",
    "                        speaker=speaker,\n",
    "                        sample_rate=self.output_sample_rate).numpy()\n",
    "\n",
    "\n",
    "    def __call__(self, context: ProcessingContext) -> ProcessingContext:\n",
    "        for segment in context.segments:\n",
    "            segment.tts_wav = self._process_sample(segment.translation, self.speaker)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9028c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "silero_tts_path = \"/home/maksim/Models/SileroModels\"\n",
    "\n",
    "tts_pipe = SileroTTSProcessor(silero_tts_path)\n",
    "\n",
    "context = tts_pipe(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0e79017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "\n",
    "\n",
    "def merge_segments_with_alignment(context: ProcessingContext, tts_sr: int) -> np.ndarray:\n",
    "    orig_audio_len = len(context.original_audio)\n",
    "    output_audio = np.array([0.0] * orig_audio_len)\n",
    "\n",
    "    for segment in context.segments:\n",
    "        segment_len = segment.end - segment.start\n",
    "        tts_wav = torchaudio.transforms.Resample(orig_freq=tts_sr, new_freq=context.sample_rate)(torch.tensor(segment.tts_wav)).numpy()\n",
    "\n",
    "        if len(tts_wav) < segment_len:\n",
    "            output_audio[segment.start:segment.start+len(tts_wav)] = tts_wav\n",
    "        else:\n",
    "            output_audio[segment.start:segment.end] = tts_wav[:segment_len]\n",
    "\n",
    "    return output_audio\n",
    "\n",
    "\n",
    "def merge_audio_video(audio: np.ndarray, sr: int, video_path: str, output_path: str):\n",
    "    audio_path = \"./temp-audios\" + \"/output.wav\"\n",
    "\n",
    "    sf.write(audio_path, audio, 16_000)\n",
    "\n",
    "    video = ffmpeg.input(video_path).video\n",
    "    audio = ffmpeg.input(audio_path).audio\n",
    "\n",
    "    ffmpeg.output(audio, video, output_path, vcodec=\"copy\", acodec=\"aac\").run()\n",
    "\n",
    "    os.remove(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc16405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "[aist#0:0/pcm_s16le @ 0x595ff3f258c0] Guessed Channel Layout: mono\n",
      "Input #0, wav, from './temp-audios/output.wav':\n",
      "  Duration: 00:03:28.56, bitrate: 256 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, 1 channels, s16, 256 kb/s\n",
      "Input #1, mov,mp4,m4a,3gp,3g2,mj2, from '/home/maksim/Repos/video-dubbing/test-videos/videoplayback.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 00:03:28.56, start: 0.000000, bitrate: 401 kb/s\n",
      "  Stream #1:0[0x1](und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 640x360 [SAR 1:1 DAR 16:9], 302 kb/s, 24 fps, 24 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #1:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 96 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> aac (native))\n",
      "  Stream #1:0 -> #0:1 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to './outp-4.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Audio: aac (LC) (mp4a / 0x6134706D), 16000 Hz, mono, fltp, 69 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 aac\n",
      "  Stream #0:1(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 640x360 [SAR 1:1 DAR 16:9], q=2-31, 302 kb/s, 24 fps, 24 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "[out#0/mp4 @ 0x595ff3f2b240] video:7708kB audio:1400kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.040146%\n",
      "size=    9203kB time=00:03:28.51 bitrate= 361.6kbits/s speed= 196x    \n",
      "[aac @ 0x595ff3f95ec0] Qavg: 37166.484\n"
     ]
    }
   ],
   "source": [
    "outp_audio = merge_segments_with_alignment(context, tts_pipe.output_sample_rate)\n",
    "\n",
    "merge_audio_video(outp_audio, 16_000, video_path, \"./outp-4.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e4e22",
   "metadata": {},
   "source": [
    "# Объединение в единый пайплайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3eaa6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessorConfig:\n",
    "    stage: str = field(repr=True)\n",
    "    model: type = field(repr=True)\n",
    "    params: dict = field(repr=True)\n",
    "\n",
    "\n",
    "cpu_config = {\n",
    "    \"pipeline\": [\n",
    "        ProcessorConfig(\n",
    "            stage=\"vad\",\n",
    "            model=SileroVADProcessor,\n",
    "            params={\n",
    "                \"model_path\": \"/home/maksim/Models/SileroVAD/snakers4-silero-vad\", \n",
    "                \"threshold\": 0.5,  \n",
    "                \"min_silence_duration_ms\": 1000, \n",
    "                \"min_speech_duration_ms\": 1000\n",
    "            }\n",
    "        ),\n",
    "        ProcessorConfig(\n",
    "            stage=\"asr\",\n",
    "            model=FasterWhisperProcessor,\n",
    "            params={\n",
    "                \"model_size_or_path\": \"/home/maksim/Models/FasterWhisper/tiny-en\", \n",
    "                \"device\": \"cpu\", \n",
    "                \"compute_type\": \"int8\"\n",
    "            }\n",
    "        ), \n",
    "        ProcessorConfig(\n",
    "            stage=\"mt\",\n",
    "            model=HelsinkiEnRuProcessor,\n",
    "            params={\n",
    "                \"model_path\": \"/home/maksim/Models/OpusEnRu\", \n",
    "                \"device\": 'cpu'\n",
    "            }\n",
    "        ),  \n",
    "        ProcessorConfig(\n",
    "            stage=\"tts\",\n",
    "            model=SileroTTSProcessor,\n",
    "            params={\n",
    "                \"model_path\": \"/home/maksim/Models/SileroModels\", \n",
    "                \"device\": 'cpu', \n",
    "                \"speaker\": \"xenia\"\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    \"temp-dir\": \"./video-dubbing-temp-dir\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbbf4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "\n",
    "\n",
    "class VideoDubber:\n",
    "    def __init__(self, config: dict):\n",
    "        self.processors = []\n",
    "\n",
    "        for processor in config[\"pipeline\"]:\n",
    "            self.processors.append(processor.model(**processor.params))\n",
    "        \n",
    "        self.temp_dir = config[\"temp-dir\"]\n",
    "    \n",
    "\n",
    "    def _extract_audio_from_mp4(self, \n",
    "                                video_path: str, \n",
    "                                target_sr: int = 16000) -> tuple[np.ndarray, int]:\n",
    "        \n",
    "        video = VideoFileClip(video_path)\n",
    "        audio: AudioFileClip = video.audio\n",
    "\n",
    "        if not os.path.exists(self.temp_dir):\n",
    "            os.makedirs(self.temp_dir)\n",
    "\n",
    "        temp_audio_path = self.temp_dir + \"original_audio.wav\"\n",
    "        audio.write_audiofile(temp_audio_path, codec='pcm_s16le', fps=target_sr)\n",
    "    \n",
    "        audio_data, sr = torchaudio.load(temp_audio_path)\n",
    "    \n",
    "        if sr != target_sr:\n",
    "            audio_data = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(audio_data)\n",
    "    \n",
    "        if audio_data.shape[0] > 1:\n",
    "            audio_data = audio_data.mean(dim=0)\n",
    "\n",
    "        os.remove(temp_audio_path)\n",
    "    \n",
    "        return audio_data.numpy(), sr\n",
    "\n",
    "\n",
    "    def _merge_audio_video(self, audio: np.ndarray, sr: int, video_path: str, output_path: str):\n",
    "        audio_path = self.temp_dir + \"/output.wav\"\n",
    "\n",
    "        sf.write(audio_path, audio, 16_000)\n",
    "\n",
    "        video = ffmpeg.input(video_path).video\n",
    "        audio = ffmpeg.input(audio_path).audio\n",
    "\n",
    "        ffmpeg.output(audio, video, output_path, vcodec=\"copy\", acodec=\"aac\").run()\n",
    "\n",
    "        os.remove(audio_path)\n",
    "    \n",
    "\n",
    "    def _merge_segments_with_alignment(self, context: ProcessingContext) -> np.ndarray:\n",
    "        orig_audio_len = len(context.original_audio)\n",
    "        output_audio = np.array([0.0] * orig_audio_len)\n",
    "    \n",
    "        tts_sr = self.processors[-1].output_sample_rate\n",
    "\n",
    "        for segment in context.segments:\n",
    "            segment_len = segment.end - segment.start\n",
    "            tts_wav = torchaudio.transforms.Resample(orig_freq=tts_sr, new_freq=context.sample_rate)(torch.tensor(segment.tts_wav)).numpy()\n",
    "\n",
    "            if len(tts_wav) < segment_len:\n",
    "                output_audio[segment.start:segment.start+len(tts_wav)] = tts_wav\n",
    "            else:\n",
    "                output_audio[segment.start:segment.end] = tts_wav[:segment_len]\n",
    "\n",
    "        return output_audio\n",
    "\n",
    "\n",
    "    def __call__(self, input_video_path: str, output_video_path: str):\n",
    "        os.makedirs(self.temp_dir, exist_ok=True)\n",
    "\n",
    "        audio, sr = self._extract_audio_from_mp4(input_video_path, target_sr=16_000)\n",
    "\n",
    "        context = ProcessingContext(original_audio=audio, sample_rate=sr, temp_dir=self.temp_dir)\n",
    "\n",
    "        for processor in self.processors:\n",
    "            context = processor(context)\n",
    "        \n",
    "        output_audio = self._merge_segments_with_alignment(context=context)\n",
    "\n",
    "        self._merge_audio_video(output_audio, sr, input_video_path, output_video_path)\n",
    "        \n",
    "        os.rmdir(self.temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f51f4f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksim/miniconda3/envs/video_dubbing/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "dubber = VideoDubber(config=cpu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a96bbe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./video-dubbing-temp-diroriginal_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "[aist#0:0/pcm_s16le @ 0x569a87b4a8c0] Guessed Channel Layout: mono\n",
      "Input #0, wav, from './video-dubbing-temp-dir/output.wav':\n",
      "  Duration: 00:03:28.56, bitrate: 256 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, 1 channels, s16, 256 kb/s\n",
      "Input #1, mov,mp4,m4a,3gp,3g2,mj2, from '/home/maksim/Repos/video-dubbing/test-videos/videoplayback.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 00:03:28.56, start: 0.000000, bitrate: 401 kb/s\n",
      "  Stream #1:0[0x1](und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 640x360 [SAR 1:1 DAR 16:9], 302 kb/s, 24 fps, 24 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #1:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 96 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> aac (native))\n",
      "  Stream #1:0 -> #0:1 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to './outp.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Audio: aac (LC) (mp4a / 0x6134706D), 16000 Hz, mono, fltp, 69 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 aac\n",
      "  Stream #0:1(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 640x360 [SAR 1:1 DAR 16:9], q=2-31, 302 kb/s, 24 fps, 24 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "[out#0/mp4 @ 0x569a87b50240] video:7708kB audio:1400kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.040146%\n",
      "size=    9203kB time=00:03:28.51 bitrate= 361.6kbits/s speed= 173x    \n",
      "[aac @ 0x569a87bbaec0] Qavg: 37166.484\n"
     ]
    }
   ],
   "source": [
    "dubber(video_path, \"./outp.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_dubbing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
