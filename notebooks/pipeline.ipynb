{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5991fa75",
   "metadata": {},
   "source": [
    "# <center> Video Dubbing Full Pipeline </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8f1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import torch \n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from moviepy import VideoFileClip, AudioFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d811c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_from_mp4(video_path: str, target_sr: int = 16000, temp_dir='./temp-audios', delete_file=True) -> tuple[np.ndarray, int]:\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio: AudioFileClip = video.audio\n",
    "    \n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "\n",
    "    temp_audio_path = temp_dir + \"/temp_audio.wav\"\n",
    "    audio.write_audiofile(temp_audio_path, codec='pcm_s16le', fps=target_sr)\n",
    "    \n",
    "    audio_data, sr = torchaudio.load(temp_audio_path)\n",
    "    \n",
    "    if sr != target_sr:\n",
    "        audio_data = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(audio_data)\n",
    "    \n",
    "    if audio_data.shape[0] > 1:\n",
    "        audio_data = audio_data.mean(dim=0)\n",
    "\n",
    "    if delete_file:\n",
    "        os.remove(temp_audio_path)\n",
    "    \n",
    "    return audio_data.numpy(), sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e870495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./temp-audios/temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "video_path = \"/home/maksim/Repos/video_dubbing/test-videos/videoplayback.mp4\"\n",
    "\n",
    "audio, sr = extract_audio_from_mp4(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d46eec",
   "metadata": {},
   "source": [
    "## Базовые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4b5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b294e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10130345",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Segment:\n",
    "    \"\"\"\n",
    "    Класс сегмента с речью в исходной аудиодорожке. \n",
    "    \"\"\"\n",
    "    start: int = field(repr=True) # Начало сегмента (индекс в исходном аудио)\n",
    "    end: int = field(repr=True) # Конец сегмента\n",
    "\n",
    "    audio: np.ndarray = field(repr=False) # Аудио сегмент\n",
    "\n",
    "    transcription: str = field(repr=True, default=None) \n",
    "    translation: str = field(repr=True, default=None) \n",
    "\n",
    "    tts_wav: np.ndarray = field(repr=False, default=None) # Озвучка "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b9512ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VadOutput:\n",
    "    \"\"\"\n",
    "    Класс возвращаемого VAD-пайплайном значения.\n",
    "    \"\"\"\n",
    "    segments: list[Segment] = field(repr=False) # Речевые сегменты в исходном аудио\n",
    "    audio: np.ndarray = field(repr=False) # Склеенные в единое аудио речевые сегменты\n",
    "    timestamps_mapping: dict[tuple[int, int], Segment] = field(repr=False) # сопоставление таймкодов склеенного аудио с сегментами исходного аудио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618198b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AsrWordOutput:\n",
    "    \"\"\"\n",
    "    Результатом транскрибации является транскрипция с word-level временными метками. \n",
    "    Данный класс - это обёртка для каждого отдельного слова транскрипции.\n",
    "    \"\"\"\n",
    "    start: int = field(repr=True)\n",
    "    end: int = field(repr=True)\n",
    "    word: str = field(repr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49dd1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADPipeline(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def __call__(self, audio, *args, **kwargs) -> VadOutput:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ASRPipeline(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def __call__(self, audio) -> list[AsrWordOutput]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class MTPipeline(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def _process_sample(self, text_en: str) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, texts_en: list[Segment]) -> list[Segment]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class TTSPipeline(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def _process_sample(self, audio: str) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, texts_ru: list[Segment], *args, **kwargs) -> list[Segment]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c436c99e",
   "metadata": {},
   "source": [
    "## VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6b07ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SileroVADPipeline(VADPipeline):\n",
    "    def __init__(self, model_path: str = \"\"):\n",
    "        \"\"\"\n",
    "        Для локальной загрузки модели, нужно сначала её скачать: git clone <silerovad repo>\n",
    "        А затем передать в качестве параметра model_path путь до корня склонированного репозитория.\n",
    "        \"\"\"\n",
    "        if model_path:\n",
    "            self.silerovad, utils = torch.hub.load(repo_or_dir=model_path,\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              source='local')\n",
    "        else:\n",
    "            self.silerovad, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                                model='silero_vad',\n",
    "                                force_reload=True, \n",
    "                                source='github') \n",
    "\n",
    "        (self.get_speech_timestamps, _, _, _, _) = utils\n",
    "\n",
    "\n",
    "    def __call__(self, audio: np.ndarray, threshold: float = 0.5,  min_silence_duration_ms=1000, min_speech_duration_ms=1000, sampling_rate=16000) -> VadOutput:\n",
    "        speech_segments: list[Segment] = []\n",
    "        speech_audio = []\n",
    "        new2old = {}\n",
    "    \n",
    "        speech_timestamps = self.get_speech_timestamps(audio, \n",
    "                                                  self.silerovad, \n",
    "                                                  threshold=threshold, \n",
    "                                                  sampling_rate=sampling_rate,\n",
    "                                                  min_silence_duration_ms=min_silence_duration_ms, \n",
    "                                                  min_speech_duration_ms=min_speech_duration_ms)\n",
    "    \n",
    "        for ts in speech_timestamps:\n",
    "            start = ts['start']\n",
    "            end = ts['end']\n",
    "            speech_segments.append(Segment(start=start, end=end, audio=audio[start:end]))\n",
    "\n",
    "\n",
    "        start_idx = 0\n",
    "\n",
    "        for segment in speech_segments:\n",
    "            new2old[(start_idx, segment.end - segment.start + start_idx)] = segment\n",
    "\n",
    "            start_idx = segment.end - segment.start + start_idx + 1\n",
    "\n",
    "            speech_audio.extend(segment.audio.tolist())\n",
    "\n",
    "        speech_audio = np.array(speech_audio)\n",
    "\n",
    "        output: VadOutput = VadOutput(segments=speech_segments, audio=speech_audio, timestamps_mapping=new2old)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f24f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_path = \"/home/maksim/Models/SileroVAD/snakers4-silero-vad\"\n",
    "\n",
    "vad_pipe = SileroVADPipeline(vad_path)\n",
    "\n",
    "vad_outp = vad_pipe(audio, sampling_rate=sr)\n",
    "\n",
    "segments = vad_outp.segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea8d68af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment(start=225312, end=308704, transcription=None, translation=None)\n"
     ]
    }
   ],
   "source": [
    "print(segments[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c1b2c",
   "metadata": {},
   "source": [
    "## ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8d2e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "class FasterWhisperPipeline(ASRPipeline):\n",
    "    sr = 16_000\n",
    "\n",
    "    def __init__(self, model_size_or_path: str = \"tiny.en\", device: str = \"cpu\", compute_type: str = \"int8\"):\n",
    "        self.model = WhisperModel(model_size_or_path=model_size_or_path, device=device, compute_type=compute_type)\n",
    "    \n",
    "    def __call__(self, audio: np.ndarray) -> list[AsrWordOutput]:\n",
    "        output = []\n",
    "\n",
    "        segments, _ = self.model.transcribe(audio, word_timestamps=True)\n",
    "        \n",
    "        for segment in segments:\n",
    "            for word in segment.words:\n",
    "                output.append(\n",
    "                    AsrWordOutput(\n",
    "                        start=int(word.start * self.sr), \n",
    "                        end=int(word.end * self.sr), \n",
    "                        word=word.word\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f34eeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_path = \"/home/maksim/Models/FasterWhisper/tiny-en\"\n",
    "\n",
    "asr_pipe = FasterWhisperPipeline(whisper_path)\n",
    "\n",
    "asr_outp = asr_pipe(vad_outp.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b77e7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_asr_outp(asr_outp: list[AsrWordOutput]):\n",
    "    \"\"\"\n",
    "    Сопоставление каждого слова из транскрипции с таймкодами исходного аудио.\n",
    "    \"\"\"\n",
    "    for word in asr_outp:\n",
    "        for interval in vad_outp.timestamps_mapping.keys():\n",
    "            if word.start >= interval[0] and word.end <= interval[1]:\n",
    "                segment = vad_outp.timestamps_mapping[interval]\n",
    "                if segment.transcription:\n",
    "                    segment.transcription += \" \" + word.word\n",
    "                else:\n",
    "                    segment.transcription = word.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c33fd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_asr_outp(asr_outp=asr_outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9fcf0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What  you're  doing  right  now  at  this  very  moment  is  killing\n"
     ]
    }
   ],
   "source": [
    "print(segments[0].transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d27f8d",
   "metadata": {},
   "source": [
    "## EN -> RU (Machine Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac641496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "class HelsinkiEnRuPipeline(MTPipeline):\n",
    "    def __init__(self, model_path: str | None = None, device: str = 'cpu'):\n",
    "        model = None\n",
    "        tokenizer = None \n",
    "        if model_path:\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        else:\n",
    "            model_hf_name = \"Helsinki-NLP/opus-mt-en-ru\"\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_hf_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_hf_name)\n",
    "        \n",
    "        self.pipe = pipeline(\n",
    "            task=\"translation\", \n",
    "            model=model, \n",
    "            tokenizer=tokenizer,\n",
    "            device=device)\n",
    "\n",
    "    def _process_sample(self, text_en: str) -> str:\n",
    "        return self.pipe(text_en)[0]['translation_text']\n",
    "\n",
    "\n",
    "    def __call__(self, segments: list[Segment]):\n",
    "        for segment in segments:\n",
    "            segment.translation = self._process_sample(segment.transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "602b1af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksim/miniconda3/envs/video_dubbing/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "mt_path = \"/home/maksim/Models/OpusEnRu\"\n",
    "\n",
    "mt_pipe = HelsinkiEnRuPipeline(model_path=mt_path)\n",
    "\n",
    "mt_pipe(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cd453d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "То, что ты делаешь прямо сейчас, это убивает\n"
     ]
    }
   ],
   "source": [
    "print(segments[0].translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3b0e2",
   "metadata": {},
   "source": [
    "## TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b9964",
   "metadata": {},
   "source": [
    "### XTTS-v2 (GPU Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8aa4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84dbf47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "class XTTSPipeline(TTSPipeline):\n",
    "    output_sampling_rate = 24_000\n",
    "\n",
    "    def __init__(self, target_spk: str | None = None, model_path: str | None = None, device: str = 'cpu', temp_dir=\"./temp-dir/tts/\"):\n",
    "        self.target_spk = target_spk\n",
    "\n",
    "        if model_path:\n",
    "            self.model = TTS(model_path=model_path, config_path=f'{model_path}/config.json').to(device)\n",
    "        else:\n",
    "            self.model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "        self.temp_dir = temp_dir\n",
    "\n",
    "\n",
    "    def _process_sample(self, text_ru: str, speaker_wav: str) -> np.ndarray:\n",
    "        return self.model.tts(text=text_ru, speaker_wav=speaker_wav, language='ru')\n",
    "\n",
    "\n",
    "    def __call__(self, segments: list[Segment]) -> list[Segment]:\n",
    "        os.makedirs(self.temp_dir, exist_ok=True)\n",
    "\n",
    "        for i, segment in enumerate(segments):\n",
    "            if self.target_spk is None:\n",
    "                audio_path = self.temp_dir + f\"{i}.wav\"\n",
    "\n",
    "                sf.write(audio_path, segment.audio, 16_000)\n",
    "\n",
    "                segment.tts_wav = self._process_sample(segment.translation, audio_path)\n",
    "\n",
    "                os.remove(audio_path)\n",
    "            else:\n",
    "                segment.tts_wav = self._process_sample(segment.translation, self.target_spk)\n",
    "\n",
    "        return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1b594",
   "metadata": {},
   "source": [
    "### Silero-TTS (CPU Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ed80060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SileroTTSPipeline(TTSPipeline):\n",
    "    output_sampling_rate = 48_000\n",
    "\n",
    "    def __init__(self, model_path: str | None = None, device: str = 'cpu'):\n",
    "        if model_path:\n",
    "            self.silero_tts, _ = torch.hub.load(repo_or_dir=model_path,\n",
    "                                     model='silero_tts',\n",
    "                                     language='ru',\n",
    "                                     speaker='v4_ru',\n",
    "                                     source='local')\n",
    "\n",
    "        else:\n",
    "            self.silero_tts, _ = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                     model='silero_tts',\n",
    "                                     language='ru',\n",
    "                                     speaker='v4_ru',\n",
    "                                     source='github')\n",
    "        \n",
    "        self.silero_tts.to(device)\n",
    "\n",
    "\n",
    "    def _process_sample(self, text_ru: str, speaker: str) -> np.ndarray:\n",
    "        return self.silero_tts.apply_tts(text=text_ru,\n",
    "                        speaker=speaker,\n",
    "                        sample_rate=self.output_sampling_rate).numpy()\n",
    "\n",
    "\n",
    "    def __call__(self, segments: list[Segment], speaker: str = \"xenia\"):\n",
    "        for segment in segments:\n",
    "            segment.tts_wav = self._process_sample(segment.translation, speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9828a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtts_path = \"/home/maksim/Models/XTTS/XTTS_mg_ft/\"\n",
    "\n",
    "tts_pipe = XTTSPipeline(model_path=xtts_path)\n",
    "\n",
    "tts_pipe(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d745c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "silero_tts_path = \"/home/maksim/Models/SileroModels\"\n",
    "\n",
    "tts_pipe = SileroTTSPipeline(silero_tts_path)\n",
    "\n",
    "tts_pipe(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41d54c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(audio, segments: list[Segment], tts: type):\n",
    "    audio_len = len(audio)\n",
    "    output_audio = np.array([0.0]*audio_len)\n",
    "\n",
    "    for segment in segments:\n",
    "        segment_tts = torch.tensor(segment.tts_wav)\n",
    "        segment_tts = torchaudio.transforms.Resample(tts.output_sampling_rate, 16_000)(segment_tts)\n",
    "        \n",
    "        for i in range(len(segment_tts)):\n",
    "            output_audio[segment.start + i] = segment_tts[i]\n",
    "        \n",
    "    return output_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03bce930",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = postprocess(audio, segments, type(tts_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbb8fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import ffmpeg\n",
    "\n",
    "\n",
    "def merge_audio_video(audio: np.ndarray, video_path: str, output_path: str):\n",
    "    audio_path = \"../outputs/tmp.wav\"\n",
    "\n",
    "    sf.write(audio_path, audio, 16_000)\n",
    "\n",
    "    video = ffmpeg.input(video_path).video\n",
    "    audio = ffmpeg.input(audio_path).audio\n",
    "\n",
    "    ffmpeg.output(audio, video, output_path, vcodec=\"copy\", acodec=\"aac\").run()\n",
    "\n",
    "    os.remove(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2aea86dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "[aist#0:0/pcm_s16le @ 0x55ff1a138a00] Guessed Channel Layout: mono\n",
      "Input #0, wav, from '../outputs/tmp.wav':\n",
      "  Duration: 00:03:28.56, bitrate: 256 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, 1 channels, s16, 256 kb/s\n",
      "Input #1, mov,mp4,m4a,3gp,3g2,mj2, from '/home/maksim/Repos/video_dubbing/test-videos/videoplayback.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 00:03:28.56, start: 0.000000, bitrate: 401 kb/s\n",
      "  Stream #1:0[0x1](und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 640x360 [SAR 1:1 DAR 16:9], 302 kb/s, 24 fps, 24 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #1:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 96 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> aac (native))\n",
      "  Stream #1:0 -> #0:1 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to '../outputs/outp-3.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Audio: aac (LC) (mp4a / 0x6134706D), 16000 Hz, mono, fltp, 69 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 aac\n",
      "  Stream #0:1(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 640x360 [SAR 1:1 DAR 16:9], q=2-31, 302 kb/s, 24 fps, 24 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "[out#0/mp4 @ 0x55ff1a13e300] video:7708kB audio:1323kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.049069%\n",
      "size=    9126kB time=00:03:28.51 bitrate= 358.5kbits/s speed= 204x    \n",
      "[aac @ 0x55ff1a1a8dc0] Qavg: 39594.801\n"
     ]
    }
   ],
   "source": [
    "merge_audio_video(result, video_path, \"../outputs/outp-3.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931db8a0",
   "metadata": {},
   "source": [
    "# Объединяем всё в единый пайплайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec35981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDubber:\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "    \n",
    "    def _extract_audio_from_mp4(self, video_path: str, target_sr: int = 16000, temp_dir='./temp-audios', delete_file=True) -> tuple[np.ndarray, int]:\n",
    "        \"\"\"\n",
    "        Извлекает аудио из MP4-файла, преобразует к моно (1 канал) и заданной частоте дискретизации.\n",
    "    \n",
    "        Args:\n",
    "            video_path: Путь к MP4-файлу.\n",
    "            target_sr: Целевая частота дискретизации (по умолчанию 16 кГц для Whisper).\n",
    "    \n",
    "        Returns:\n",
    "            Аудио-данные в виде np.array и частоту дискретизации.\n",
    "        \"\"\"\n",
    "        # 1. Извлекаем аудио из видео с помощью moviepy\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio: AudioFileClip = video.audio\n",
    "    \n",
    "        # 2. Сохраняем временный WAV-файл (moviepy не возвращает напрямую np.array)\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "\n",
    "        temp_audio_path = temp_dir + \"/temp_audio.wav\"\n",
    "        audio.write_audiofile(temp_audio_path, codec='pcm_s16le', fps=target_sr)\n",
    "    \n",
    "        # 3. Загружаем аудио \n",
    "        audio_data, sr = torchaudio.load(temp_audio_path)\n",
    "    \n",
    "        if sr != target_sr:\n",
    "            audio_data = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(audio_data)\n",
    "    \n",
    "        if audio_data.shape[0] > 1:\n",
    "            audio_data = audio_data.mean(dim=0)\n",
    "\n",
    "        if delete_file:\n",
    "            os.remove(temp_audio_path)\n",
    "    \n",
    "        return audio_data.numpy(), sr\n",
    "    \n",
    "\n",
    "    def _merge_segments(self, audio, segments: list[Segment]):\n",
    "        audio_len = len(audio)\n",
    "        output_audio = np.array([0.0]*audio_len)\n",
    "\n",
    "        for segment in segments:\n",
    "            segment_tts = torch.tensor(segment.tts_wav)\n",
    "            segment_tts = torchaudio.transforms.Resample(SileroTTSPipeline.output_sampling_rate, 16_000)(segment_tts)\n",
    "        \n",
    "            for i in range(len(segment_tts)):\n",
    "                output_audio[segment.start + i] = segment_tts[i]\n",
    "        \n",
    "        return output_audio\n",
    "\n",
    "\n",
    "    def _merge_audio_with_video(self, audio, input_video_path, output_video_path):\n",
    "        audio_path = \"./tmp.wav\"\n",
    "\n",
    "        sf.write(audio_path, audio, 16_000)\n",
    "\n",
    "        video = ffmpeg.input(input_video_path).video\n",
    "        audio = ffmpeg.input(audio_path).audio\n",
    "\n",
    "        ffmpeg.output(audio, video, output_video_path, vcodec=\"copy\", acodec=\"aac\", ).run()\n",
    "    \n",
    "\n",
    "    def __call__(self, input_video_path: str, output_video_path: str):\n",
    "        audio, sr = self._extract_audio_from_mp4(input_video_path)\n",
    "        \n",
    "        segments: list[Segment] = []\n",
    "\n",
    "        # Vad\n",
    "        vad_pipe = None\n",
    "        if self.config[\"vad\"][\"model\"] == \"silero\":\n",
    "            vad_pipe = SileroVADPipeline(model_path=self.config[\"vad\"][\"model_path\"])\n",
    "        else:\n",
    "            raise ValueError(\"Vad model should be in list: ['silero']\")\n",
    "        \n",
    "        segments = vad_pipe(audio)\n",
    "        \n",
    "        # ASR\n",
    "        asr_pipe = None\n",
    "        if self.config[\"asr\"][\"model\"] == \"whisper\":\n",
    "            asr_pipe = WhisperPipeline(model_path=self.config[\"asr\"][\"model_path\"], device=self.config[\"mt\"][\"device\"])\n",
    "        else:\n",
    "            raise ValueError(\"ASR model should be in list: ['whisper']\")\n",
    "        \n",
    "        asr_pipe(segments)\n",
    "\n",
    "        # MT\n",
    "        mt_pipe = None\n",
    "        if self.config[\"mt\"][\"model\"] == \"opus-en-ru\":\n",
    "            mt_pipe = HelsinkiEnRuPipeline(model_path=self.config[\"mt\"][\"model_path\"], device=self.config[\"mt\"][\"device\"])\n",
    "        else:\n",
    "            raise ValueError(\"MT model should be in list: ['opus-en-ru']\")\n",
    "\n",
    "        mt_pipe(segments)\n",
    "        \n",
    "        # TTS\n",
    "        tts_pipe = None\n",
    "        if self.config[\"tts\"][\"model\"] == \"xtts\":\n",
    "            tts_pipe = XTTSPipeline(self.config[\"tts\"][\"params\"][\"speaker_wav\"], self.config[\"tts\"][\"model_path\"], device=self.config[\"tts\"][\"device\"])\n",
    "        elif self.config[\"tts\"][\"model\"] == \"silero\":\n",
    "            tts_pipe = SileroTTSPipeline(model_path=self.config[\"tts\"][\"model_path\"], device=self.config[\"tts\"][\"device\"])\n",
    "        else:\n",
    "            raise ValueError(\"TTS model should be in list: ['silero', 'xtts']\")\n",
    "        \n",
    "        tts_pipe(segments)\n",
    "\n",
    "        output_audio = self._merge_segments(audio, segments)\n",
    "        self._merge_audio_with_video(output_audio, input_video_path, output_video_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0159e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dubbing_config = {\n",
    "    \"vad\": {\n",
    "        \"model\": \"silero\",\n",
    "        \"model_path\": \"/home/maksim/Models/SileroVAD/snakers4-silero-vad\",\n",
    "        \"device\": \"cpu\",\n",
    "        \"params\": {\n",
    "            \"threshold\": 0.5,\n",
    "            \"min_silence_duration_ms\": 1000,\n",
    "            \"min_speech_duration_ms\": 1000\n",
    "        }\n",
    "    },\n",
    "    \"asr\": {\n",
    "        \"model\": \"whisper\",\n",
    "        \"device\": \"cpu\",\n",
    "        \"model_path\": \"/home/maksim/Models/Whisper/tiny\"\n",
    "    },\n",
    "    \"mt\": {\n",
    "        \"model\": \"opus-en-ru\",\n",
    "        \"device\": \"cpu\",\n",
    "        \"model_path\": \"/home/maksim/Models/OpusEnRu\"\n",
    "    },\n",
    "    \"tts\": {\n",
    "        \"model\": \"silero\",\n",
    "        \"model_path\": \"/home/maksim/Models/SileroModels\",\n",
    "        \"device\": \"cpu\",\n",
    "        \"params\": {\n",
    "            \"speaker\": \"xenia\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6b9c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dubber = VideoDubber(video_dubbing_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c929ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./temp-audios/temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/home/maksim/miniconda3/envs/video_dubbing/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "/home/maksim/miniconda3/envs/video_dubbing/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n",
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "[aist#0:0/pcm_s16le @ 0x55c927db2a00] Guessed Channel Layout: mono\n",
      "Input #0, wav, from './tmp.wav':\n",
      "  Duration: 00:03:28.56, bitrate: 256 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, 1 channels, s16, 256 kb/s\n",
      "Input #1, mov,mp4,m4a,3gp,3g2,mj2, from '/home/maksim/Repos/video_dubbing/test-videos/videoplayback.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 00:03:28.56, start: 0.000000, bitrate: 401 kb/s\n",
      "  Stream #1:0[0x1](und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 640x360 [SAR 1:1 DAR 16:9], 302 kb/s, 24 fps, 24 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #1:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 96 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> aac (native))\n",
      "  Stream #1:0 -> #0:1 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to './outp-3.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Audio: aac (LC) (mp4a / 0x6134706D), 16000 Hz, mono, fltp, 69 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 aac\n",
      "  Stream #0:1(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 640x360 [SAR 1:1 DAR 16:9], q=2-31, 302 kb/s, 24 fps, 24 tbr, 12288 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "[out#0/mp4 @ 0x55c927db8300] video:7708kB audio:1434kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.036339%\n",
      "size=    9237kB time=00:03:28.51 bitrate= 362.9kbits/s speed= 182x    \n",
      "[aac @ 0x55c927e22dc0] Qavg: 38135.738\n"
     ]
    }
   ],
   "source": [
    "video_dubber(\"/home/maksim/Repos/video_dubbing/test-videos/videoplayback.mp4\", \"./outp-3.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_dubbing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
